{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd47804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/unlearning-relearn/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8729581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "IMPROVED CONFIGURATION FOR STABLE RELEARNING EVALUATION\n",
      "======================================================================\n",
      "Key changes to reduce variance:\n",
      "  1. Using FULL WMDP-bio test set (~500 questions) instead of 100 random\n",
      "  2. Fixed eval set (shuffle=False) - same questions each evaluation\n",
      "  3. Evaluate every 20 steps (was 1 - too noisy)\n",
      "  4. Larger batch size (2) for stable gradients\n",
      "  5. More relearning samples (500) per epoch\n",
      "  6. More eval samples for WikiText and MMLU\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Configuration - IMPROVED FOR STABILITY\n",
    "# ============================================\n",
    "\n",
    "bio_dir = \"/workspace/transcoder-ablation-pipeline/unlearning/bio\"\n",
    "\n",
    "# Unlearned model paths\n",
    "UNLEARNED_MODELS = {\n",
    "    # \"GradDiff\": f\"{bio_dir}/graddiff-bio-only\",\n",
    "    \"MaxEntropy\": f\"{bio_dir}/maxentropy-bio-only\",\n",
    "    \"RMU\": f\"{bio_dir}/rmu-bio-only\",\n",
    "    \"NPO\": f\"{bio_dir}/npo-bio-only\",\n",
    "}\n",
    "\n",
    "# IMPROVED Relearning attack config for stability\n",
    "RELEARN_CONFIG = {\n",
    "    # Relearning hyperparams\n",
    "    \"n_samples\": 500,        # INCREASED: More samples per epoch = better learning\n",
    "    \"n_epochs\": 3,           # Can reduce epochs if using more samples\n",
    "    \"lr\": 1e-5,\n",
    "    \"batch_size\": 2,         # INCREASED: Larger batches = more stable gradients\n",
    "    \"max_length\": 512,\n",
    "    \n",
    "    # Evaluation settings - KEY CHANGES FOR STABILITY\n",
    "    \"eval_every\": 20,        # CHANGED: Evaluate less frequently (was 1, very noisy)\n",
    "    \"eval_samples\": None,    # CHANGED: Use FULL test set (was 100, too small)\n",
    "    \"eval_shuffle\": False,   # CHANGED: Use same questions each time (was True, caused variance)\n",
    "    \"eval_seed\": 42,         # Fixed seed for reproducibility\n",
    "    \n",
    "    # Additional stability options\n",
    "    \"use_full_wmdp\": True,   # Use full WMDP test set (~500 questions)\n",
    "    \"wikitext_samples\": 500, # More samples for perplexity (was 100)\n",
    "    \"mmlu_samples\": 500,     # More samples for MMLU (was 100)\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IMPROVED CONFIGURATION FOR STABLE RELEARNING EVALUATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Key changes to reduce variance:\")\n",
    "print(f\"  1. Using FULL WMDP-bio test set (~500 questions) instead of 100 random\")\n",
    "print(f\"  2. Fixed eval set (shuffle=False) - same questions each evaluation\")\n",
    "print(f\"  3. Evaluate every {RELEARN_CONFIG['eval_every']} steps (was 1 - too noisy)\")\n",
    "print(f\"  4. Larger batch size ({RELEARN_CONFIG['batch_size']}) for stable gradients\")\n",
    "print(f\"  5. More relearning samples ({RELEARN_CONFIG['n_samples']}) per epoch\")\n",
    "print(f\"  6. More eval samples for WikiText and MMLU\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9592e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEARN_CONFIG = {\n",
    "    # Relearning hyperparams - REDUCED to prevent overfitting\n",
    "    \"n_samples\": 60,         # Match papers (was 500)\n",
    "    \"n_epochs\": 1,           # Match papers (was 3)\n",
    "    \"lr\": 5e-6,              # Gentler (was 1e-5)\n",
    "    \"batch_size\": 2,\n",
    "    \"max_length\": 512,\n",
    "    \n",
    "    # Evaluation settings\n",
    "    \"eval_every\": 5,\n",
    "    \"eval_samples\": None,    # Use FULL WMDP test set\n",
    "    \"eval_shuffle\": False,   # Fixed order\n",
    "    \"eval_seed\": 42,\n",
    "    \n",
    "    # Additional \n",
    "    \"wikitext_samples\": 200,\n",
    "    \"mmlu_samples\": 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bd091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define HF_TOKEN as a Python variable\n",
    "HF_TOKEN = \"\"\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec92d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Utility Functions\n",
    "# ============================================\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory between model runs.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "def load_model(model_path: str):\n",
    "    \"\"\"Load model and tokenizer.\"\"\"\n",
    "    torch_dtype = \"auto\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_forget_corpus(n_samples: int, min_len: int = 50):\n",
    "    \"\"\"Load bio forget corpus samples.\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"cais/wmdp-bio-forget-corpus\", # change for forget corpus\n",
    "        split=\"train\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    \n",
    "    # Filter by length and sample\n",
    "    texts = [x['text'] for x in dataset if len(x['text']) > min_len]\n",
    "    \n",
    "    # Random sample\n",
    "    if n_samples < len(texts):\n",
    "        texts = random.sample(texts, n_samples)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} forget samples\")\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f605b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_forget_corpus(n_samples: int = None, min_len: int = 50):\n",
    "    \"\"\"Load bio forget corpus samples. If n_samples is None, load all.\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"cais/wmdp-bio-forget-corpus\",\n",
    "        split=\"train\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    \n",
    "    texts = [x['text'] for x in dataset if len(x['text']) > min_len]\n",
    "    \n",
    "    if n_samples and n_samples < len(texts):\n",
    "        texts = random.sample(texts, n_samples)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} forget samples\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_wmdp_bio_eval(n_questions: int = None, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    Load WMDP-bio multiple choice questions.\n",
    "    \n",
    "    Args:\n",
    "        n_questions: Number of questions to return (None = all)\n",
    "        shuffle: If True, randomly sample questions. If False, take first N.\n",
    "    \n",
    "    Returns list of dicts with 'question', 'choices', 'answer' keys.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"cais/wmdp\",\n",
    "        \"wmdp-bio\",\n",
    "        split=\"test\",\n",
    "        token=HF_TOKEN\n",
    "    )\n",
    "    \n",
    "    questions = []\n",
    "    for item in dataset:\n",
    "        questions.append({\n",
    "            'question': item['question'],\n",
    "            'choices': item['choices'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "    \n",
    "    if n_questions and n_questions < len(questions):\n",
    "        if shuffle:\n",
    "            questions = random.sample(questions, n_questions)\n",
    "        else:\n",
    "            questions = questions[:n_questions]\n",
    "    \n",
    "    print(f\"Loaded {len(questions)} WMDP-bio questions (shuffle={shuffle})\")\n",
    "    return questions\n",
    "\n",
    "\n",
    "def load_wikitext_eval(n_samples: int = 100, min_len: int = 50):\n",
    "    \"\"\"\n",
    "    Load wikitext samples for perplexity evaluation.\n",
    "    \n",
    "    Returns list of text strings.\n",
    "    \"\"\"\n",
    "    raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    \n",
    "    texts = [x['text'] for x in raw_data if len(x['text']) > min_len]\n",
    "    \n",
    "    if n_samples and n_samples < len(texts):\n",
    "        texts = random.sample(texts, n_samples)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} wikitext samples for perplexity eval\")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def format_mcq_prompt(question: str, choices: list) -> str:\n",
    "    prompt = f\"Question: {question}\\n\\n\"\n",
    "    for i, choice in enumerate(choices):\n",
    "        prompt += f\"{chr(65+i)}. {choice}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def evaluate_wmdp_accuracy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    questions: list,\n",
    "    batch_size: int = 4,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate model accuracy on WMDP-bio MCQ.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    answer_tokens = [tokenizer.encode(f\" {chr(65+i)}\", add_special_tokens=False)[-1] for i in range(4)]\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(questions), batch_size):\n",
    "            batch = questions[i:i+batch_size]\n",
    "            prompts = [format_mcq_prompt(q['question'], q['choices']) for q in batch]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            answer_logits = logits[:, answer_tokens]\n",
    "            predictions = answer_logits.argmax(dim=-1)\n",
    "            \n",
    "            for j, q in enumerate(batch):\n",
    "                if predictions[j].item() == q['answer']:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def evaluate_perplexity(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    texts: list,\n",
    "    max_length: int = 512,\n",
    "    batch_size: int = 4,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate perplexity on a list of texts (e.g., wikitext).\n",
    "    \n",
    "    Lower perplexity = better language modeling = retained capabilities.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            n_tokens = attention_mask.sum().item()\n",
    "            \n",
    "            # Accumulate loss * tokens (to compute weighted average)\n",
    "            total_loss += outputs.loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "def load_mmlu_eval(n_questions: int = None, shuffle: bool = False, seed: int = 42):\n",
    "    \"\"\"Load MMLU questions for general knowledge evaluation.\"\"\"\n",
    "    dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"test\")\n",
    "    \n",
    "    questions = []\n",
    "    for item in dataset:\n",
    "        questions.append({\n",
    "            'question': item['question'],\n",
    "            'choices': item['choices'],\n",
    "            'answer': item['answer'],\n",
    "        })\n",
    "    \n",
    "    if n_questions and n_questions < len(questions):\n",
    "        if shuffle:\n",
    "            rng = random.Random(seed)\n",
    "            questions = rng.sample(questions, n_questions)\n",
    "        else:\n",
    "            questions = questions[:n_questions]\n",
    "    \n",
    "    print(f\"Loaded {len(questions)} MMLU questions\")\n",
    "    return questions\n",
    "def evaluate_mmlu_accuracy(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    questions: list,\n",
    "    batch_size: int = 4,\n",
    ") -> float:\n",
    "    \"\"\"Evaluate model accuracy on MMLU. Same format as WMDP.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    answer_tokens = [tokenizer.encode(f\" {chr(65+i)}\", add_special_tokens=False)[-1] for i in range(4)]\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(questions), batch_size):\n",
    "            batch = questions[i:i+batch_size]\n",
    "            prompts = [format_mcq_prompt(q['question'], q['choices']) for q in batch]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            answer_logits = logits[:, answer_tokens]\n",
    "            predictions = answer_logits.argmax(dim=-1)\n",
    "            \n",
    "            for j, q in enumerate(batch):\n",
    "                if predictions[j].item() == q['answer']:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "            \n",
    "            del inputs, outputs, logits, answer_logits, predictions\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a2c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Relearning Attack\n",
    "# ============================================\n",
    "\n",
    "def run_relearning_attack(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    forget_corpus: list,\n",
    "    eval_questions: list,\n",
    "    wikitext_eval: list = None,\n",
    "    mmlu_questions: list = None,\n",
    "    n_samples_per_epoch: int = 100,\n",
    "    n_epochs: int = 1,\n",
    "    lr: float = 1e-5,\n",
    "    batch_size: int = 2,\n",
    "    max_length: int = 512,\n",
    "    eval_every: int = 5,\n",
    ") -> dict:\n",
    "    \"\"\"Run relearning attack with WMDP, perplexity, and MMLU tracking.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    results = {\n",
    "        'steps': [],\n",
    "        'loss_curve': [],\n",
    "        'accuracy_curve': [],\n",
    "        'perplexity_curve': [],\n",
    "        'mmlu_curve': [],\n",
    "    }\n",
    "    \n",
    "    step = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{n_epochs} ===\")\n",
    "        \n",
    "        epoch_texts = random.sample(forget_corpus, min(n_samples_per_epoch, len(forget_corpus)))\n",
    "        \n",
    "        n_batches = len(epoch_texts) // batch_size\n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm(range(n_batches), desc=f\"Epoch {epoch + 1}\")\n",
    "        \n",
    "        for batch_idx in pbar:\n",
    "            start_idx = batch_idx * batch_size\n",
    "            batch_texts = epoch_texts[start_idx:start_idx + batch_size]\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            step += 1\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Critical: Clean up training tensors\n",
    "            del inputs, outputs\n",
    "            \n",
    "            # Periodic cache cleanup during training\n",
    "            if batch_idx % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            if step % eval_every == 0:\n",
    "                # Synchronize before evaluation\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                results['steps'].append(step)\n",
    "                results['loss_curve'].append(loss.item())\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                # WMDP accuracy\n",
    "                acc = evaluate_wmdp_accuracy(model, tokenizer, eval_questions)\n",
    "                results['accuracy_curve'].append(acc)\n",
    "                \n",
    "                # Wikitext perplexity\n",
    "                ppl = None\n",
    "                if wikitext_eval:\n",
    "                    ppl = evaluate_perplexity(model, tokenizer, wikitext_eval)\n",
    "                    results['perplexity_curve'].append(ppl)\n",
    "                \n",
    "                # MMLU accuracy\n",
    "                mmlu_acc = None\n",
    "                if mmlu_questions:\n",
    "                    mmlu_acc = evaluate_mmlu_accuracy(model, tokenizer, mmlu_questions)\n",
    "                    results['mmlu_curve'].append(mmlu_acc)\n",
    "                \n",
    "                status = f\"  Step {step}: WMDP={acc:.1%}\"\n",
    "                if ppl:\n",
    "                    status += f\", PPL={ppl:.2f}\"\n",
    "                if mmlu_acc:\n",
    "                    status += f\", MMLU={mmlu_acc:.1%}\"\n",
    "                print(status)\n",
    "                \n",
    "                # Critical: Synchronize and clean up after evaluation\n",
    "                torch.cuda.synchronize()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                model.train()\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        print(f\"Epoch {epoch + 1} avg loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Clean up between epochs\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    results['final_loss'] = results['loss_curve'][-1] if results['loss_curve'] else avg_loss\n",
    "    results['final_accuracy'] = results['accuracy_curve'][-1] if results['accuracy_curve'] else None\n",
    "    results['final_perplexity'] = results['perplexity_curve'][-1] if results['perplexity_curve'] else None\n",
    "    results['final_mmlu'] = results['mmlu_curve'][-1] if results['mmlu_curve'] else None\n",
    "    results['total_steps'] = step\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main Processing Function\n",
    "# ============================================\n",
    "\n",
    "def process_single_model(\n",
    "    method_name: str,\n",
    "    model_path: str,\n",
    "    forget_corpus: list,\n",
    "    eval_questions: list,\n",
    "    wikitext_eval: list,\n",
    "    mmlu_questions: list,\n",
    "    output_dir: str,\n",
    "    config: dict,\n",
    "    original_accuracy: float,\n",
    "    original_perplexity: float,\n",
    "    original_mmlu: float,\n",
    "    save_relearned_model: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"Process a single unlearned model with full evaluation.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {method_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model, tokenizer = load_model(model_path)\n",
    "    \n",
    "    # Evaluate BEFORE relearning\n",
    "    print(\"Evaluating before relearning...\")\n",
    "    model.eval()\n",
    "    before_acc = evaluate_wmdp_accuracy(model, tokenizer, eval_questions)\n",
    "    before_ppl = evaluate_perplexity(model, tokenizer, wikitext_eval)\n",
    "    before_mmlu = evaluate_mmlu_accuracy(model, tokenizer, mmlu_questions)\n",
    "    print(f\"  Before: WMDP={before_acc:.1%}, PPL={before_ppl:.2f}, MMLU={before_mmlu:.1%}\")\n",
    "    \n",
    "    # Clean up after initial eval\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Run relearning attack\n",
    "    print(f\"\\nRunning relearning attack...\")\n",
    "    attack_results = run_relearning_attack(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        forget_corpus=forget_corpus,\n",
    "        eval_questions=eval_questions,\n",
    "        wikitext_eval=wikitext_eval,\n",
    "        mmlu_questions=mmlu_questions,\n",
    "        n_samples_per_epoch=config['n_samples'],\n",
    "        n_epochs=config['n_epochs'],\n",
    "        lr=config['lr'],\n",
    "        batch_size=config['batch_size'],\n",
    "        max_length=config['max_length'],\n",
    "        eval_every=config['eval_every'],\n",
    "    )\n",
    "    \n",
    "    # Clean up after relearning\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Evaluate AFTER relearning\n",
    "    print(\"\\nEvaluating after relearning...\")\n",
    "    model.eval()\n",
    "    after_acc = evaluate_wmdp_accuracy(model, tokenizer, eval_questions)\n",
    "    after_ppl = evaluate_perplexity(model, tokenizer, wikitext_eval)\n",
    "    after_mmlu = evaluate_mmlu_accuracy(model, tokenizer, mmlu_questions)\n",
    "    print(f\"  After: WMDP={after_acc:.1%}, PPL={after_ppl:.2f}, MMLU={after_mmlu:.1%}\")\n",
    "    \n",
    "    # Save relearned model\n",
    "    relearned_model_path = None\n",
    "    if save_relearned_model:\n",
    "        relearned_model_path = os.path.join(output_dir, f\"{method_name}-relearned\")\n",
    "        print(f\"\\nSaving relearned model to {relearned_model_path}...\")\n",
    "        os.makedirs(relearned_model_path, exist_ok=True)\n",
    "        model.save_pretrained(relearned_model_path)\n",
    "        tokenizer.save_pretrained(relearned_model_path)\n",
    "    \n",
    "    # Compute recovery rate\n",
    "    if original_accuracy > before_acc:\n",
    "        recovery_rate = (after_acc - before_acc) / (original_accuracy - before_acc)\n",
    "        recovery_rate = max(0, min(1, recovery_rate))\n",
    "    else:\n",
    "        recovery_rate = 0.0\n",
    "    \n",
    "    print(f\"  Recovery rate: {recovery_rate:.1%}\")\n",
    "    \n",
    "    results = {\n",
    "        'method': method_name,\n",
    "        'before_accuracy': before_acc,\n",
    "        'after_accuracy': after_acc,\n",
    "        'before_perplexity': before_ppl,\n",
    "        'after_perplexity': after_ppl,\n",
    "        'before_mmlu': before_mmlu,\n",
    "        'after_mmlu': after_mmlu,\n",
    "        'recovery_rate': recovery_rate,\n",
    "        'original_accuracy': original_accuracy,\n",
    "        'original_perplexity': original_perplexity,\n",
    "        'original_mmlu': original_mmlu,\n",
    "        'relearned_model_path': relearned_model_path,\n",
    "        **attack_results,\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(output_dir, f\"{method_name}_relearning_results.json\")\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=lambda x: float(x) if hasattr(x, 'item') else x)\n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "    \n",
    "    del model, tokenizer\n",
    "    clear_memory()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def get_original_metrics(\n",
    "    base_model_name: str = \"google/gemma-2-2b\",\n",
    "    eval_questions: list = None,\n",
    "    wikitext_eval: list = None,\n",
    "    mmlu_questions: list = None,\n",
    ") -> tuple:\n",
    "    \"\"\"Measure WMDP accuracy, wikitext perplexity, and MMLU accuracy of original model.\"\"\"\n",
    "    model, tokenizer = load_model(base_model_name)\n",
    "    \n",
    "    accuracy = evaluate_wmdp_accuracy(model, tokenizer, eval_questions)\n",
    "    perplexity = evaluate_perplexity(model, tokenizer, wikitext_eval)\n",
    "    mmlu_acc = evaluate_mmlu_accuracy(model, tokenizer, mmlu_questions)\n",
    "    \n",
    "    del model, tokenizer\n",
    "    clear_memory()\n",
    "    \n",
    "    return accuracy, perplexity, mmlu_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d88d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "Loaded 24432 forget samples\n",
      "Loaded 1273 WMDP-bio questions (shuffle=False)\n",
      "Loaded 200 wikitext samples for perplexity eval\n",
      "Loaded 200 MMLU questions\n",
      "\n",
      "Measuring original model metrics on google/gemma-2-2b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original WMDP-bio accuracy: 55.5%\n",
      "Original Wikitext perplexity: 62.82\n",
      "Original MMLU accuracy: 37.5%\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Main Execution - Load Data with Improved Settings\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "\n",
    "# Load ALL forget corpus for relearning\n",
    "forget_corpus = load_forget_corpus(n_samples=None)\n",
    "\n",
    "# KEY CHANGE: Load FULL test set with FIXED order (no shuffle)\n",
    "eval_questions = load_wmdp_bio_eval(\n",
    "    n_questions=RELEARN_CONFIG['eval_samples'],  # None = use all\n",
    "    shuffle=RELEARN_CONFIG['eval_shuffle']        # False = same order each time\n",
    ")\n",
    "\n",
    "# Load more samples for perplexity and MMLU for stability\n",
    "wikitext_eval = load_wikitext_eval(n_samples=RELEARN_CONFIG['wikitext_samples'])\n",
    "mmlu_questions = load_mmlu_eval(\n",
    "    n_questions=RELEARN_CONFIG['mmlu_samples'],\n",
    "    shuffle=False,  # Fixed set\n",
    "    seed=RELEARN_CONFIG['eval_seed']\n",
    ")\n",
    "\n",
    "# Get original model metrics on SAME eval sets\n",
    "BASE_MODEL = \"google/gemma-2-2b\"\n",
    "print(f\"\\nMeasuring original model metrics on {BASE_MODEL}...\")\n",
    "original_accuracy, original_perplexity, original_mmlu = get_original_metrics(\n",
    "    base_model_name=BASE_MODEL,\n",
    "    eval_questions=eval_questions,\n",
    "    wikitext_eval=wikitext_eval,\n",
    "    mmlu_questions=mmlu_questions,\n",
    ")\n",
    "print(f\"Original WMDP-bio accuracy: {original_accuracy:.1%}\")\n",
    "print(f\"Original Wikitext perplexity: {original_perplexity:.2f}\")\n",
    "print(f\"Original MMLU accuracy: {original_mmlu:.1%}\")\n",
    "\n",
    "# Clear memory before processing unlearned models\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "850e1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(bio_dir, \"relearn_results\")\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61076bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: MaxEntropy\n",
      "============================================================\n",
      "Loading model from /workspace/transcoder-ablation-pipeline/unlearning/bio/maxentropy-bio-only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating before relearning...\n",
      "  Before: WMDP=25.4%, PPL=63.66, MMLU=32.0%\n",
      "\n",
      "Running relearning attack...\n",
      "\n",
      "=== Epoch 1/1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 5/30 [01:42<14:56, 35.85s/it, loss=4.0800] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 5: WMDP=38.2%, PPL=68.19, MMLU=33.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 10/30 [03:25<11:58, 35.91s/it, loss=1.8797]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 10: WMDP=50.8%, PPL=73.67, MMLU=34.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 15/30 [05:08<08:59, 35.96s/it, loss=2.6311]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 15: WMDP=52.6%, PPL=8.13, MMLU=33.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 20/30 [06:51<06:00, 36.01s/it, loss=2.1996]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 20: WMDP=52.9%, PPL=52.22, MMLU=33.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 25/30 [08:34<03:00, 36.05s/it, loss=1.8183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 25: WMDP=52.8%, PPL=24.13, MMLU=33.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 30/30 [10:17<00:00, 20.59s/it, loss=2.2397]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 30: WMDP=52.5%, PPL=21.23, MMLU=32.5%\n",
      "Epoch 1 avg loss: 3.1208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating after relearning...\n",
      "  After: WMDP=52.5%, PPL=21.23, MMLU=32.5%\n",
      "\n",
      "Saving relearned model to /workspace/transcoder-ablation-pipeline/unlearning/bio/relearn_results/MaxEntropy-relearned...\n",
      "  Recovery rate: 90.1%\n",
      "Results saved to: /workspace/transcoder-ablation-pipeline/unlearning/bio/relearn_results/MaxEntropy_relearning_results.json\n",
      "\n",
      "============================================================\n",
      "Processing: RMU\n",
      "============================================================\n",
      "Loading model from /workspace/transcoder-ablation-pipeline/unlearning/bio/rmu-bio-only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating before relearning...\n",
      "  Before: WMDP=48.4%, PPL=65.47, MMLU=33.0%\n",
      "\n",
      "Running relearning attack...\n",
      "\n",
      "=== Epoch 1/1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 5/30 [01:43<14:59, 35.99s/it, loss=2.1479]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 5: WMDP=54.8%, PPL=75.29, MMLU=34.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 10/30 [03:25<11:57, 35.89s/it, loss=2.0702]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 10: WMDP=53.5%, PPL=84.10, MMLU=35.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 15/30 [05:08<08:59, 36.00s/it, loss=2.1875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 15: WMDP=53.5%, PPL=90.28, MMLU=35.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 20/30 [06:51<06:00, 36.07s/it, loss=1.9241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 20: WMDP=53.1%, PPL=93.38, MMLU=35.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 25/30 [08:35<03:00, 36.04s/it, loss=1.8649]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 25: WMDP=53.1%, PPL=95.18, MMLU=36.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 30/30 [10:18<00:00, 20.60s/it, loss=2.0869]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 30: WMDP=52.9%, PPL=94.22, MMLU=36.5%\n",
      "Epoch 1 avg loss: 2.2788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating after relearning...\n",
      "  After: WMDP=52.9%, PPL=94.22, MMLU=36.5%\n",
      "\n",
      "Saving relearned model to /workspace/transcoder-ablation-pipeline/unlearning/bio/relearn_results/RMU-relearned...\n",
      "  Recovery rate: 64.4%\n",
      "Results saved to: /workspace/transcoder-ablation-pipeline/unlearning/bio/relearn_results/RMU_relearning_results.json\n",
      "\n",
      "============================================================\n",
      "Processing: NPO\n",
      "============================================================\n",
      "Loading model from /workspace/transcoder-ablation-pipeline/unlearning/bio/npo-bio-only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating before relearning...\n",
      "  Before: WMDP=41.9%, PPL=27.80, MMLU=27.5%\n",
      "\n",
      "Running relearning attack...\n",
      "\n",
      "=== Epoch 1/1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|█▋        | 5/30 [01:42<14:58, 35.96s/it, loss=2.4006]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 5: WMDP=43.3%, PPL=28.03, MMLU=30.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 10/30 [03:25<11:57, 35.87s/it, loss=2.3478]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 10: WMDP=43.2%, PPL=30.61, MMLU=30.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 15/30 [05:08<08:59, 35.98s/it, loss=2.6416]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 15: WMDP=43.8%, PPL=31.17, MMLU=31.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 20/30 [06:51<06:00, 36.04s/it, loss=2.6392]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 20: WMDP=44.5%, PPL=30.05, MMLU=31.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 25/30 [08:34<03:00, 36.02s/it, loss=2.9520]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 25: WMDP=45.2%, PPL=28.12, MMLU=32.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 30/30 [10:17<00:00, 20.59s/it, loss=2.4631]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 30: WMDP=45.6%, PPL=27.86, MMLU=32.5%\n",
      "Epoch 1 avg loss: 2.4329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating after relearning...\n",
      "  After: WMDP=45.6%, PPL=27.86, MMLU=32.5%\n",
      "\n",
      "Saving relearned model to /workspace/transcoder-ablation-pipeline/unlearning/bio/relearn_results/NPO-relearned...\n",
      "  Recovery rate: 27.7%\n",
      "Results saved to: /workspace/transcoder-ablation-pipeline/unlearning/bio/relearn_results/NPO_relearning_results.json\n"
     ]
    }
   ],
   "source": [
    "all_results = {}\n",
    "    \n",
    "for method_name, model_path in UNLEARNED_MODELS.items():\n",
    "    if os.path.exists(model_path):\n",
    "        results = process_single_model(\n",
    "            method_name=method_name,\n",
    "            model_path=model_path,\n",
    "            forget_corpus=forget_corpus,\n",
    "            eval_questions=eval_questions,\n",
    "            wikitext_eval=wikitext_eval,\n",
    "            mmlu_questions=mmlu_questions,\n",
    "            output_dir=output_dir,\n",
    "            config=RELEARN_CONFIG,\n",
    "            original_accuracy=original_accuracy,\n",
    "            original_perplexity=original_perplexity,\n",
    "            original_mmlu=original_mmlu,\n",
    "        )\n",
    "        all_results[method_name] = results\n",
    "        clear_memory()\n",
    "    else:\n",
    "        print(f\"\\nSkipping {method_name}: {model_path} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f359d43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RELEARNING ATTACK RESULTS\n",
      "======================================================================\n",
      "\n",
      "Config: 60 samples/epoch, 1 epoch(s), lr=5e-06\n",
      "Eval every 5 steps\n",
      "\n",
      "Method           Before      After   Recovery  MMLU Before MMLU After\n",
      "----------------------------------------------------------------------\n",
      "MaxEntropy       25.4%     52.5%     90.1%       32.0%     32.5%\n",
      "RMU              48.4%     52.9%     64.4%       33.0%     36.5%\n",
      "NPO              41.9%     45.6%     27.7%       27.5%     32.5%\n",
      "\n",
      "Summary saved to: /workspace/transcoder-ablation-pipeline/unlearning/bio/relearn_results/relearning_summary.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RELEARNING ATTACK RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nConfig: {RELEARN_CONFIG['n_samples']} samples/epoch, {RELEARN_CONFIG['n_epochs']} epoch(s), lr={RELEARN_CONFIG['lr']}\")\n",
    "print(f\"Eval every {RELEARN_CONFIG['eval_every']} steps\")\n",
    "\n",
    "print(f\"\\n{'Method':<12} {'Before':>10} {'After':>10} {'Recovery':>10} {'MMLU Before':>12} {'MMLU After':>10}\")\n",
    "print(\"-\" * 70)\n",
    "for method, results in all_results.items():\n",
    "    before = results['before_accuracy']\n",
    "    after = results['after_accuracy']\n",
    "    recovery = results['recovery_rate']\n",
    "    before_mmlu = results.get('before_mmlu', 0)\n",
    "    after_mmlu = results.get('after_mmlu', 0)\n",
    "    print(f\"{method:<12} {before:>9.1%} {after:>9.1%} {recovery:>9.1%} {before_mmlu:>11.1%} {after_mmlu:>9.1%}\")\n",
    "\n",
    "# Save summary\n",
    "summary_path = os.path.join(output_dir, \"relearning_summary.json\")\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=lambda x: float(x) if hasattr(x, 'item') else x)\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning-relearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
