{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5181c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'huggingface_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      3\u001b[39m login(token=\u001b[33m\"\u001b[39m\u001b[33mhf_YOoKDVDDSSTjIUmFuMuMkYSyjjPUQeRqAq\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'huggingface_hub'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "login(token=\"\")\n",
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6585ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from RMU utils\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random\n",
    "random.seed(0)\n",
    "from datasets import load_dataset\n",
    "\n",
    "################################\n",
    "##### Activation functions #####\n",
    "################################\n",
    "\n",
    "def forward_with_cache(model, inputs, module, no_grad=True):\n",
    "    # define a tensor with the size of our cached activations\n",
    "    cache = []\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            cache.append(output[0])\n",
    "        else:\n",
    "            cache.append(output)\n",
    "        return None \n",
    "    \n",
    "    hook_handle = module.register_forward_hook(hook)\n",
    "    \n",
    "    if no_grad:\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    else:\n",
    "        _ = model(**inputs)\n",
    "        \n",
    "    hook_handle.remove()\n",
    "\n",
    "    return cache[0]\n",
    "    \n",
    "#######################################\n",
    "##### Model and data loading code #####\n",
    "#######################################\n",
    "\n",
    "\n",
    "def get_params(model, layer_ids, param_ids):\n",
    "    params = []\n",
    "    for layer_id in layer_ids:\n",
    "        for i, p in enumerate(model.model.layers[layer_id].parameters()):\n",
    "            if i in param_ids:\n",
    "                params.append(p)\n",
    "    return params\n",
    "\n",
    "\n",
    "def load_model(model_name_or_path):\n",
    "    torch_dtype = \"auto\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch_dtype,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path, trust_remote_code=True, use_fast=False\n",
    "    )\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.mask_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.sep_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.cls_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_data(forget_corpora, retain_corpora, min_len=50, max_len=2000, batch_size=4):\n",
    "    def get_dataset(name):\n",
    "        data = []\n",
    "        if name == \"wikitext\":\n",
    "            raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "            for x in raw_data:\n",
    "                if len(x['text']) > min_len:\n",
    "                    data.append(str(x['text']))\n",
    "        else:\n",
    "            assert os.getenv(\"HF_TOKEN\"), \"HF_TOKEN is not set\"\n",
    "            # Support both full names like \"cais/wmdp-bio-forget-corpus\" and short names like \"bio-forget-corpus\"\n",
    "            dataset_name = name if \"/\" in name else f\"cais/wmdp-{name}\"\n",
    "            dataset = load_dataset(dataset_name, split=\"train\", token=os.getenv(\"HF_TOKEN\"))\n",
    "            for line in dataset:\n",
    "                if len(line['text']) > min_len:\n",
    "                    data.append(str(line['text']))\n",
    "        data = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "        return data\n",
    "\n",
    "    return (\n",
    "        [get_dataset(c) for c in forget_corpora],\n",
    "        [get_dataset(c) for c in retain_corpora]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7962d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMU code\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import tqdm as tqdm\n",
    "\n",
    "def run_rmu(\n",
    "    updated_model,\n",
    "    frozen_model,\n",
    "    tokenizer,\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    args,\n",
    "):\n",
    "    rmu_config = vars(args)\n",
    "    print(\"====rmu Config====\")\n",
    "    print(\"\\n\".join(f\"{k}={v}\" for k,v in rmu_config.items()))\n",
    "    print(\"=====\")\n",
    "\n",
    "    updated_model = updated_model.train()\n",
    "    params = get_params(updated_model, args.layer_ids, args.param_ids)\n",
    "    optimizer = AdamW(params, lr=args.lr)\n",
    "    frozen_module = eval(\n",
    "        args.module_str.format(model_name=\"frozen_model\", layer_id=args.layer_id)\n",
    "    )\n",
    "    updated_module = eval(\n",
    "        args.module_str.format(model_name=\"updated_model\", layer_id=args.layer_id)\n",
    "    )\n",
    "\n",
    "    control_vectors_list = []\n",
    "    for i in range(len(forget_data_list)):\n",
    "        random_vector = torch.rand(1,1, updated_model.config.hidden_size, dtype=updated_model.dtype, device=updated_model.device)\n",
    "        control_vec = random_vector / torch.norm(random_vector) * args.steering_coeff_list[i]\n",
    "        control_vectors_list.append(control_vec)\n",
    "\n",
    "    num_batches = min(\n",
    "        args.max_num_batches,\n",
    "        min([len(f) for f in forget_data_list]),\n",
    "        min([len(r) for r in retain_data_list]),\n",
    "    )\n",
    "    \n",
    "    truncation_side = tokenizer.truncation_side\n",
    "    tokenizer.truncation_side=\"right\"\n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(f\"======= Epoch {epoch} =======\")\n",
    "        with tqdm.tqdm(total=num_batches) as pbar:\n",
    "            for idx in range(num_batches):\n",
    "                topic_idx = idx % len(forget_data_list)\n",
    "                batch_idx = idx // len(forget_data_list)\n",
    "                control_vec = control_vectors_list[topic_idx]\n",
    "                unlearn_batch = forget_data_list[topic_idx][batch_idx]\n",
    "                retain_batch = retain_data_list[topic_idx][batch_idx]\n",
    "\n",
    "                # Unlearning loss\n",
    "                max_length = 512 if topic_idx == 0 else 768\n",
    "                unlearn_inputs = tokenizer(\n",
    "                    unlearn_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "                )\n",
    "                updated_forget_activations = forward_with_cache(\n",
    "                    updated_model, unlearn_inputs, module=updated_module, no_grad=False\n",
    "                ).to(updated_model.device)\n",
    "\n",
    "                unlearn_loss = torch.nn.functional.mse_loss(\n",
    "                    updated_forget_activations, control_vec\n",
    "                )\n",
    "\n",
    "                # Retain loss\n",
    "                retain_inputs = tokenizer(\n",
    "                    retain_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "                ).to(updated_model.device)\n",
    "                updated_retain_activations = forward_with_cache(\n",
    "                    updated_model, retain_inputs, module=updated_module, no_grad=False\n",
    "                ).to(updated_model.device)\n",
    "                frozen_retain_activations = forward_with_cache(\n",
    "                    frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                ).to(updated_model.device)\n",
    "\n",
    "                retain_loss = torch.nn.functional.mse_loss(\n",
    "                    updated_retain_activations, frozen_retain_activations\n",
    "                )\n",
    "                retain_loss *= args.alpha[topic_idx]\n",
    "\n",
    "                # Update model\n",
    "                loss = unlearn_loss + retain_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(f\"loss: {loss.item():.4g} | unlearn_loss: {unlearn_loss.item():.4g} | retain_loss: {retain_loss.item():.4g} | param_change: {params[0].grad.abs().mean().item():.4g}\")\n",
    "                \n",
    "                # ======= Logging ======\n",
    "                if args.verbose:\n",
    "                    frozen_forget_activations = forward_with_cache(frozen_model, unlearn_inputs, module=frozen_module, no_grad=True).to(updated_model.device)\n",
    "                    unlearn_cosine= torch.nn.functional.cosine_similarity(updated_forget_activations, frozen_forget_activations, dim=-1).mean()\n",
    "                    retain_cosine = torch.nn.functional.cosine_similarity(updated_retain_activations, frozen_retain_activations, dim=-1).mean()\n",
    "                    \n",
    "                    print(f\"unlearn_cosine_sim={unlearn_cosine.item()}\")\n",
    "                    print(f\"retain_cosine_sim={retain_cosine.item()}\")\n",
    "                    print(f\"Topic {topic_idx} updated_forget_activations.norm=\",torch.mean(updated_forget_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "                    print(f\"Topic {topic_idx} frozen_forget_activations.norm=\",torch.mean(frozen_forget_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "                    print(f\"Topic {topic_idx} updated_retain_activations.norm=\",torch.mean(updated_retain_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "                    print(f\"Topic {topic_idx} frozen_retain_activations.norm=\",torch.mean(frozen_retain_activations.norm(dim=-1).mean(dim=1), dim=0).item())\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "    # Save model\n",
    "    if args.output_dir:\n",
    "        path = args.output_dir\n",
    "    else:\n",
    "        date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        path = f\"rmu-models/{args.model_name_or_path}_alpha-{args.alpha}_batches-{num_batches}_layer-{args.layer_id}_{date}\"\n",
    "    updated_model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "    print(f\"Saved model to {path}\")\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    ### Model arguments\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\", type=str, default= \"google/gemma-2-2b\" # changed to gemma\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--module_str\", type=str, default=\"{model_name}.model.layers[{layer_id}]\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", type=str, default=\"./my_unlearned_models-rmu\"\n",
    "    )\n",
    "    ### Data arguments\n",
    "    parser.add_argument(\n",
    "        \"--retain_corpora\",\n",
    "        type=str,\n",
    "        default=\"wikitext,wikitext\",\n",
    "        help=\"comma-separated list of corpora to retain\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--forget_corpora\",\n",
    "        type=str,\n",
    "        default=\"bio-forget-corpus,cyber-forget-corpus\",\n",
    "        help=\"comma-separated list of corpora to forget\",\n",
    "    )\n",
    "    ### rmu hyperparameters\n",
    "    parser.add_argument(\"--alpha\", type=str, default=\"100,100\", help=\"retain weight\")\n",
    "    parser.add_argument(\n",
    "        \"--steering_coeffs\",\n",
    "        type=str,\n",
    "        default=\"20,20\",\n",
    "        help=\"Steer vector weight in order of topic\",\n",
    "    )\n",
    "    parser.add_argument(\"--lr\", type=float, default=5e-5, help=\"learning rate\")\n",
    "    parser.add_argument(\"--min_len\", type=int, default=0)\n",
    "    parser.add_argument(\"--max_len\", type=int, default=2000)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--max_num_batches\", type=int, default=80)\n",
    "    parser.add_argument(\"--layer_id\", type=int, default=7, help=\"layer to unlearn\")\n",
    "    parser.add_argument(\"--layer_ids\", type=str, default=\"5,6,7\", help=\"update layers\")\n",
    "    parser.add_argument(\"--param_ids\", type=str, default=\"6\", help=\"update params\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Logging the activations norms and cosine at each step\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.retain_corpora = args.retain_corpora.split(\",\")\n",
    "    args.forget_corpora = args.forget_corpora.split(\",\")\n",
    "    args.steering_coeff_list = [float(c) for c in args.steering_coeffs.split(\",\")]\n",
    "    args.alpha = [float(c) for c in args.alpha.split(\",\")]\n",
    "    args.layer_ids = [int(layer_id) for layer_id in args.layer_ids.split(\",\")]\n",
    "    args.param_ids = [int(param_id) for param_id in args.param_ids.split(\",\")]\n",
    "    return args \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300716bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']  # Prevents argparse from reading notebook args\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "SEED = args.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b6d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_notebook(\n",
    "    model_name_or_path=\"google/gemma-2-2b\",\n",
    "    module_str=\"{model_name}.model.layers[{layer_id}]\",\n",
    "    output_dir=\"./my_unlearned_models-rmu\",\n",
    "    retain_corpora=None,\n",
    "    forget_corpora=None,\n",
    "    alpha=None,\n",
    "    steering_coeffs=None,\n",
    "    lr=5e-5,\n",
    "    min_len=0,\n",
    "    max_len=2000,\n",
    "    batch_size=4,\n",
    "    max_num_batches=80,\n",
    "    layer_id=7,\n",
    "    layer_ids=None,\n",
    "    param_ids=None,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"Notebook-friendly args without argparse\"\"\"\n",
    "    class Args:\n",
    "        pass\n",
    "    \n",
    "    args = Args()\n",
    "    \n",
    "    # Model arguments\n",
    "    args.model_name_or_path = model_name_or_path\n",
    "    args.module_str = module_str\n",
    "    args.output_dir = output_dir  # Use the parameter!\n",
    "    \n",
    "    # Data arguments\n",
    "    args.retain_corpora = retain_corpora if retain_corpora is not None else [\"wikitext\", \"wikitext\"]\n",
    "    args.forget_corpora = forget_corpora if forget_corpora is not None else [\"bio-forget-corpus\", \"cyber-forget-corpus\"]\n",
    "    \n",
    "    # RMU hyperparameters\n",
    "    args.alpha = alpha if alpha is not None else [100.0, 100.0]\n",
    "    args.steering_coeff_list = steering_coeffs if steering_coeffs is not None else [20.0, 20.0]\n",
    "    args.lr = lr\n",
    "    args.min_len = min_len\n",
    "    args.max_len = max_len\n",
    "    args.batch_size = batch_size\n",
    "    args.max_num_batches = max_num_batches\n",
    "    args.layer_id = layer_id\n",
    "    args.layer_ids = layer_ids if layer_ids is not None else [5, 6, 7]\n",
    "    args.param_ids = param_ids if param_ids is not None else [6]\n",
    "    args.seed = seed\n",
    "    args.verbose = verbose\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89358bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_max_entropy(\n",
    "    updated_model,\n",
    "    frozen_model,\n",
    "    tokenizer,\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    args,\n",
    "):\n",
    "    max_entropy_config = vars(args)\n",
    "    print(\"====MaxEntropy Config====\")\n",
    "    print(\"\\n\".join(f\"{k}={v}\" for k,v in max_entropy_config.items()))\n",
    "    print(\"=====\")\n",
    "\n",
    "    updated_model = updated_model.train()\n",
    "    params = get_params(updated_model, args.layer_ids, args.param_ids)\n",
    "    optimizer = AdamW(params, lr=args.lr)\n",
    "    frozen_module = eval(\n",
    "        args.module_str.format(model_name=\"frozen_model\", layer_id=args.layer_id)\n",
    "    )\n",
    "    updated_module = eval(\n",
    "        args.module_str.format(model_name=\"updated_model\", layer_id=args.layer_id)\n",
    "    )\n",
    "\n",
    "    num_batches = min(\n",
    "        args.max_num_batches,\n",
    "        min([len(f) for f in forget_data_list]),\n",
    "        min([len(r) for r in retain_data_list]),\n",
    "    )\n",
    "    \n",
    "    truncation_side = tokenizer.truncation_side\n",
    "    tokenizer.truncation_side=\"right\"\n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(f\"======= Epoch {epoch} =======\")\n",
    "        with tqdm.tqdm(total=num_batches) as pbar:\n",
    "            for idx in range(num_batches):\n",
    "                topic_idx = idx % len(forget_data_list)\n",
    "                batch_idx = idx // len(forget_data_list)\n",
    "                unlearn_batch = forget_data_list[topic_idx][batch_idx]\n",
    "                retain_batch = retain_data_list[topic_idx][batch_idx]\n",
    "\n",
    "                # Clear GPU cache before each step\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # Unlearning loss - Maximum Entropy (memory efficient)\n",
    "                max_length = 512 if topic_idx == 0 else 768\n",
    "                unlearn_inputs = tokenizer(\n",
    "                    unlearn_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n",
    "                ).to(updated_model.device)\n",
    "                \n",
    "                # Get logits from the model\n",
    "                outputs = updated_model(**unlearn_inputs, labels=unlearn_inputs[\"input_ids\"])\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Memory-efficient entropy computation using logsumexp trick\n",
    "                # H = log(vocab_size) - mean(max_logit) + mean(logsumexp - max_logit) approximately\n",
    "                # Or more precisely: H = logsumexp(logits) - sum(softmax * logits)\n",
    "                \n",
    "                # Compute in chunks to save memory\n",
    "                attention_mask = unlearn_inputs[\"attention_mask\"]\n",
    "                \n",
    "                # Use log_softmax and compute entropy without storing full probs tensor\n",
    "                with torch.amp.autocast('cuda'):  # Use mixed precision\n",
    "                    log_probs = torch.nn.functional.log_softmax(logits.float(), dim=-1)\n",
    "                    # Entropy: H = -sum(p * log(p)) = -sum(exp(log_p) * log_p)\n",
    "                    entropy = -(log_probs.exp() * log_probs).sum(dim=-1)  # (batch_size, seq_len)\n",
    "                \n",
    "                # Clean up immediately\n",
    "                del logits, log_probs, outputs\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # Average over sequence and batch (only on non-padding tokens)\n",
    "                masked_entropy = entropy * attention_mask\n",
    "                avg_entropy = masked_entropy.sum() / attention_mask.sum()\n",
    "                \n",
    "                # Maximize entropy = minimize negative entropy\n",
    "                unlearn_loss = -avg_entropy\n",
    "                \n",
    "                del entropy, masked_entropy\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # Retain loss\n",
    "                retain_inputs = tokenizer(\n",
    "                    retain_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "                ).to(updated_model.device)\n",
    "                updated_retain_activations = forward_with_cache(\n",
    "                    updated_model, retain_inputs, module=updated_module, no_grad=False\n",
    "                ).to(updated_model.device)\n",
    "                frozen_retain_activations = forward_with_cache(\n",
    "                    frozen_model, retain_inputs, module=frozen_module, no_grad=True\n",
    "                ).to(updated_model.device)\n",
    "\n",
    "                retain_loss = torch.nn.functional.mse_loss(\n",
    "                    updated_retain_activations, frozen_retain_activations\n",
    "                )\n",
    "                retain_loss *= args.alpha[topic_idx]\n",
    "\n",
    "                # Update model\n",
    "                loss = unlearn_loss + retain_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                print(f\"loss: {loss.item():.4g} | unlearn_loss: {unlearn_loss.item():.4g} | retain_loss: {retain_loss.item():.4g} | avg_entropy: {avg_entropy.item():.4g} | param_change: {params[0].grad.abs().mean().item():.4g}\")\n",
    "                \n",
    "                # Clean up\n",
    "                del updated_retain_activations, frozen_retain_activations, retain_inputs, unlearn_inputs\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                # ======= Logging ======\n",
    "                if args.verbose:\n",
    "                    print(f\"Topic {topic_idx} avg_entropy={avg_entropy.item()}\")\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "    # Save model\n",
    "    if args.output_dir:\n",
    "        path = args.output_dir\n",
    "    else:\n",
    "        date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        path = f\"maxent-models/{args.model_name_or_path}_alpha-{args.alpha}_batches-{num_batches}_layer-{args.layer_id}_{date}\"\n",
    "    updated_model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "    print(f\"Saved model to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad7cff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Difference Unlearning\n",
      "Loss: L = -L_forget + alpha * L_retain\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Union\n",
    "def run_grad_diff(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    forget_data_list: List[str],\n",
    "    retain_data_list: List[str],\n",
    "    lr: float = 1e-5,\n",
    "    num_epochs: int = 5,\n",
    "    alpha: float = 1.0,\n",
    "    max_length: int = 512,\n",
    "    output_dir: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Gradient Difference unlearning.\n",
    "    \n",
    "    Loss: L = -L_forget + alpha * L_retain\n",
    "    \n",
    "    Args:\n",
    "        model: The model to unlearn from\n",
    "        tokenizer: Tokenizer for the model\n",
    "        forget_data_list: List of text samples to forget\n",
    "        retain_data_list: List of text samples to retain\n",
    "        lr: Learning rate (default 1e-5)\n",
    "        num_epochs: Number of epochs (default 5)\n",
    "        alpha: Weight for retain loss (default 1.0)\n",
    "        max_length: Maximum sequence length\n",
    "        output_dir: Directory to save the unlearned model\n",
    "        \n",
    "    Returns:\n",
    "        The unlearned model\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    print(\"====Gradient Difference Config====\")\n",
    "    print(f\"lr={lr}, epochs={num_epochs}, alpha={alpha}, max_length={max_length}\")\n",
    "    print(f\"forget_samples={len(forget_data_list)}, retain_samples={len(retain_data_list)}\")\n",
    "    print(\"===================================\")\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    num_batches = min(len(forget_data_list), len(retain_data_list))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"======= Epoch {epoch + 1}/{num_epochs} =======\")\n",
    "        \n",
    "        with tqdm.tqdm(total=num_batches, desc=f\"Epoch {epoch + 1}\") as pbar:\n",
    "            for idx in range(num_batches):\n",
    "                # Get samples\n",
    "                forget_text = forget_data_list[idx]\n",
    "                retain_text = retain_data_list[idx]\n",
    "                \n",
    "                if isinstance(forget_text, str):\n",
    "                    forget_text = [forget_text]\n",
    "                if isinstance(retain_text, str):\n",
    "                    retain_text = [retain_text]\n",
    "                \n",
    "                # Tokenize\n",
    "                forget_inputs = tokenizer(\n",
    "                    forget_text, return_tensors=\"pt\", \n",
    "                    padding=True, truncation=True, max_length=max_length\n",
    "                ).to(device)\n",
    "                \n",
    "                retain_inputs = tokenizer(\n",
    "                    retain_text, return_tensors=\"pt\",\n",
    "                    padding=True, truncation=True, max_length=max_length\n",
    "                ).to(device)\n",
    "                \n",
    "                # Forget loss (CE)\n",
    "                forget_outputs = model(**forget_inputs, labels=forget_inputs[\"input_ids\"])\n",
    "                forget_loss = forget_outputs.loss\n",
    "                \n",
    "                # Retain loss (CE)\n",
    "                retain_outputs = model(**retain_inputs, labels=retain_inputs[\"input_ids\"])\n",
    "                retain_loss = retain_outputs.loss\n",
    "                \n",
    "                # Grad Diff: L = -L_forget + alpha * L_retain\n",
    "                loss = -forget_loss + alpha * retain_loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'forget': f'{forget_loss.item():.4f}',\n",
    "                    'retain': f'{retain_loss.item():.4f}',\n",
    "                })\n",
    "    \n",
    "    # Save\n",
    "    if output_dir:\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Saved model to {output_dir}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Gradient Difference Unlearning\")\n",
    "    print(\"Loss: L = -L_forget + alpha * L_retain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83567ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args = get_args_notebook(output_dir=\"./rmu-bio-only\",\n",
    "#     forget_corpora=[\"bio-forget-corpus\"],  # Only bio\n",
    "#     retain_corpora=[\"wikitext\"])\n",
    "\n",
    "# SEED = args.seed\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# frozen_model, tokenizer = load_model(args.model_name_or_path)\n",
    "# updated_model, tokenizer = load_model(args.model_name_or_path)\n",
    "# forget_data_list, retain_data_list = get_data(\n",
    "#   args.forget_corpora,\n",
    "#   args.retain_corpora,\n",
    "#   args.min_len,\n",
    "#   args.max_len,\n",
    "#   args.batch_size,\n",
    "# )\n",
    "# run_rmu(\n",
    "#   updated_model,\n",
    "#   frozen_model,\n",
    "#   tokenizer,\n",
    "#   forget_data_list,\n",
    "#   retain_data_list,\n",
    "#   args,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd5b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Clear memory first\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# args = get_args_notebook(\n",
    "#     output_dir=\"./maxentropy-bio-only\",\n",
    "#     forget_corpora=[\"bio-forget-corpus\"],  # Only bio\n",
    "#     retain_corpora=[\"wikitext\"],\n",
    "#     batch_size=2,  # Reduced from 4\n",
    "#     max_num_batches=80\n",
    "# )\n",
    "\n",
    "# SEED = args.seed\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# frozen_model, tokenizer = load_model(args.model_name_or_path)\n",
    "# updated_model, tokenizer = load_model(args.model_name_or_path)\n",
    "# forget_data_list, retain_data_list = get_data(\n",
    "#   args.forget_corpora,\n",
    "#   args.retain_corpora,\n",
    "#   args.min_len,\n",
    "#   args.max_len,\n",
    "#   args.batch_size,\n",
    "# )\n",
    "# run_max_entropy(\n",
    "#   updated_model,\n",
    "#   frozen_model,\n",
    "#   tokenizer,\n",
    "#   forget_data_list,\n",
    "#   retain_data_list,\n",
    "#   args,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498c1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e7878f586640a29f86d4748b3201e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Gradient Difference Config====\n",
      "lr=5e-05, epochs=1, alpha=100.0, max_length=512\n",
      "forget_samples=12221, retain_samples=1446\n",
      "===================================\n",
      "======= Epoch 1/1 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|███████   | 1015/1446 [35:07<15:05,  2.10s/it, loss=288.2296, forget=45.8257, retain=3.3406] "
     ]
    }
   ],
   "source": [
    "# Clear memory first\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "args = get_args_notebook(\n",
    "    output_dir=\"./graddiff-bio-only\",\n",
    "    forget_corpora=[\"bio-forget-corpus\"],  # Only bio\n",
    "    retain_corpora=[\"wikitext\"],\n",
    "    batch_size=2,\n",
    "    max_num_batches=80\n",
    ")\n",
    "\n",
    "SEED = args.seed\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "model, tokenizer = load_model(args.model_name_or_path)\n",
    "forget_data_list, retain_data_list = get_data(\n",
    "  args.forget_corpora,\n",
    "  args.retain_corpora,\n",
    "  args.min_len,\n",
    "  args.max_len,\n",
    "  args.batch_size,\n",
    ")\n",
    "\n",
    "# Flatten the nested list structure from get_data\n",
    "# get_data returns [[batches for corpus1], [batches for corpus2]]\n",
    "# run_grad_diff expects a flat list of batches\n",
    "forget_flat = [batch for corpus in forget_data_list for batch in corpus]\n",
    "retain_flat = [batch for corpus in retain_data_list for batch in corpus]\n",
    "\n",
    "run_grad_diff(\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  forget_data_list=forget_flat,\n",
    "  retain_data_list=retain_flat,\n",
    "  lr=args.lr,\n",
    "  num_epochs=1,\n",
    "  alpha=args.alpha[0],  # Use first alpha value\n",
    "  max_length=512,\n",
    "  output_dir=args.output_dir,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcoder-ablation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
